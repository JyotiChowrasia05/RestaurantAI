{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPshEe6canO3bElATSintgQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JyotiChowrasia05/RestaurantAI/blob/main/RestaurantAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "id": "0mzB0n9n27YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "id": "NieIz0hJpCWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install OpenAI"
      ],
      "metadata": {
        "id": "GGlfvtyl3BFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install typing_extensions"
      ],
      "metadata": {
        "id": "eKC84adq3Dfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBODRv7Dqd1w",
        "outputId": "ee25da9d-5e5a-4d1e-b8e2-96a0847d00f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dotenv\n",
            "  Downloading dotenv-0.0.5.tar.gz (2.4 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NbgsvDniiGM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import streamlit as st\n",
        "import langchain\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "w9lVKlyyh1uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.retrievers import AzureCognitiveSearchRetriever\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "import json\n",
        "import os, sys\n",
        "import hashlib\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "USMZm1BbAVZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"a35e69b894d64657ac8fe96a1e98a7d5\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ncus-lfai-nprd-01.openai.azure.com/\"\n",
        "os.environ[\"AZURE_COGNITIVE_SEARCH_SERVICE_NAME\"] = \"lfai-ir-search-service\"\n",
        "os.environ[\"AZURE_COGNITIVE_SEARCH_INDEX_NAME\"] = \"ir-vector-index\"\n",
        "os.environ[\"AZURE_COGNITIVE_SEARCH_API_KEY\"] = \"GFhhonWdwKTTxx1hlEQ98erC4pBqErNcgbOVYcAdNWAzSeCwaydv\"\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\""
      ],
      "metadata": {
        "id": "gmd7zs38zGA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = AzureCognitiveSearchRetriever(content_key=\"chunk\", top_k=2, index_name = \"ir-vector-index\")"
      ],
      "metadata": {
        "id": "pp_0-Sv_1zxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = AzureChatOpenAI(\n",
        "          openai_api_version=\"2023-05-15\",\n",
        "          azure_deployment=\"gpt-35-turbo-16k\",\n",
        "          temperature=0\n",
        "  )"
      ],
      "metadata": {
        "id": "5TCyaqjV_ymo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables=[\"cuisine\"],\n",
        "    template= \"I want to Open a Resaurant for {cuisine} food\"\n",
        ")\n",
        "name_chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
        "\n",
        "#prompt_template_name.format(cuisine=\"Italian\")\n",
        "\n",
        "# prompt_template_items = PromptTemplate(\n",
        "#     input_variables = ['restaurant_name'],\n",
        "#     template = \"Suggest some Menu Item for {restaurant_name}\"\n",
        "# )\n",
        "# items_chain = LLMChain(llm=llm, prompt=prompt_template_items)\n",
        "\n",
        "allchains = SimpleSequentialChain(chains=[name_chain])"
      ],
      "metadata": {
        "id": "MCeV9chdbWgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = allchains.run('Indian')\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Wkpl102QjVfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mychain.run('German')"
      ],
      "metadata": {
        "id": "MFTFzppghI35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [file for file in glob.glob(\"../somefolder/*\")]\n",
        "for file_name in files:\n",
        "    with io.open(file_name, 'rb') as image_file:\n",
        "        content = image_file.read()"
      ],
      "metadata": {
        "id": "LRS9zV5yDjbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "files = os.listdir('./sample_data')\n",
        "\n",
        "# print(len(arr))\n",
        "# for file in files:\n",
        "#   print(file)"
      ],
      "metadata": {
        "id": "GGGA0AnHFWeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr_txt = [t for t in os.listdir('./sample_data') if t.endswith(\".txt\")]\n",
        "arr_csv = [c for c in os.listdir('./sample_data') if c.endswith(\".csv\")]\n",
        "arr_pdf = [p for p in os.listdir('./sample_data') if p.endswith(\".pdf\")]\n",
        "\n",
        "print(arr_txt)\n",
        "print(arr_csv)\n",
        "print(arr_pdf)"
      ],
      "metadata": {
        "id": "frdJ4pyWIGn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_files(file):\n",
        "  print(\"Total Text files are:\",file)\n",
        "\n",
        "def get_csv_files(file):\n",
        "  print(\"Total CSV files are:\",file)\n",
        "\n",
        "def get_pdf_files(file):\n",
        "  print(\"Total pdf files are:\",file)\n",
        "\n",
        "for file in os.listdir('./sample_data'):\n",
        "  if file.endswith(\".txt\"):\n",
        "    get_text_files(file)\n",
        "    #print(file)\n",
        "  if file.endswith(\".csv\"):\n",
        "    get_csv_files(file)\n",
        "    #print(file)\n",
        "  if file.endswith(\".pdf\"):\n",
        "    get_pdf_files(file)\n",
        "    #print(file)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y_1_GDAfJGzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Mr. Jyoti like eating Samosa. Mr. Sagar like eating Vada Pav\")\n",
        "print(doc[11])"
      ],
      "metadata": {
        "id": "KXdNAPU8gJyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import textfileext as tf ,pdffileext as pf\n",
        "\n",
        "path = 'C:/Users/jyoti.chowrasia/js/handson_apps/sample_files/'\n",
        "hr_qns=[]\n",
        "\n",
        "def get_text_files(file):\n",
        "    text_arr=[]\n",
        "    text_file_obj = open(file,'r',encoding='cp437')\n",
        "    text_doc = text_file_obj.readline()\n",
        "    text_doc_sentence = sent_tokenize(text_doc)\n",
        "    text_file_obj.close()\n",
        "\n",
        "    for i in range(len(text_doc_sentence)):\n",
        "        #response = \"Q\"+ str(i+1) + \": \" + file_sent[i]\n",
        "        response = \"Q: \"+ text_doc_sentence[i]\n",
        "        text_arr.append(response)\n",
        "    return text_arr\n",
        "\n",
        "def get_pdf_file(file):\n",
        "    pdf_arr=[]\n",
        "    pdf_file_obj = open(file,'rb')\n",
        "    pdf_doc = PyPDF2.PdfReader(pdf_file_obj)\n",
        "    page_len = len(pdf_doc.pages)\n",
        "    pdf_doc_page_obj = pdf_doc.pages[7]\n",
        "    pdf_doc_page_extract = pdf_doc_page_obj.extract_text()\n",
        "    #print(page_extract)\n",
        "    pdf_file_obj.close()\n",
        "    pdf_doc_sentence = sent_tokenize(pdf_doc_page_extract)\n",
        "    #print(pdf_doc_sentence)\n",
        "    for i in range(len(pdf_doc_sentence)):\n",
        "        #response = \"Q\"+ str(i+1) + \": \" + file_sent[i]\n",
        "        response = \"Q: \"+ pdf_doc_sentence[i]\n",
        "        pdf_arr.append(response)\n",
        "        #print(arr)\n",
        "    return pdf_arr\n",
        "\n",
        "for file in os.listdir(path):\n",
        "    if file.endswith('.txt'):\n",
        "       #res = tf.get_text_files(path + file)\n",
        "       res = get_text_files(path + file)\n",
        "       hr_qns.append(res)\n",
        "\n",
        "    if file.endswith('.pdf'):\n",
        "       #res = pf.get_pdf_file(path + file)\n",
        "       res = get_pdf_file(path + file)\n",
        "       hr_qns.append(res)\n",
        "\n",
        "    #if file.endswith('.csv'):\n",
        "    #  get_csv_files(path + file)\n",
        "\n",
        "hr_qns_path = path + \"Hr_QnS.txt\"\n",
        "\n",
        "with open(hr_qns_path, \"w+\",encoding='utf-8') as txt_file:\n",
        "    for line in hr_qns:\n",
        "        txt_file.write(\" \".join(line) + \"\\n\")\n",
        "\n",
        "print(f\"The HR QnS has been written to {hr_qns_path}.\")\n",
        "\n",
        "\n",
        "#print(hr_qns[1])"
      ],
      "metadata": {
        "id": "4-xGBWsoBAKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dotenv"
      ],
      "metadata": {
        "id": "mjzMRcUMxEKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "css = \"\"\"\n",
        "  <style>\n",
        "    .chat-message{\n",
        "      padding: 1.5rem; border-radius:0.5rem;margin-bottom: 1rem;display:flex\n",
        "    }\n",
        "    .chat-message.user{\n",
        "      background-color: #475063;\n",
        "    }\n",
        "    .chat-message.avtar{\n",
        "      width:15%\n",
        "    }\n",
        "  </style>\n",
        "\"\"\"\n",
        "bot_template='''\n",
        "  <div class='chat-messaage bot'>\n",
        "    <div class = 'avatar'>\n",
        "      <img src='https://i.ibb.co/qWBwpNb/Photo-logo-5.png' style='max-height:78px;max-width:78px;border-radius:50%'>\n",
        "    </div>\n",
        "  </div>\n",
        "  <!-- <div class='message'>{{msg}}</div>  -->\n",
        "'''\n",
        "user_template='''\n",
        "  <div class='chat-message user'>\n",
        "    <div class='avatar'>\n",
        "      <img src='https://i.ibb.co/rdZC7LZ/Photo-logo-1.png' style='max-height:78px;max-width:78px;border-radius:50%'>\n",
        "    </div>\n",
        "  </div>\n",
        "  <div class='message'>{{msg}}</div>\n",
        "'''"
      ],
      "metadata": {
        "id": "x8kCPU-R3veb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.embeddings import AzureOpenAIEmbeddings\n",
        "from langchain.vectorstores.azuresearch import AzureSearch\n",
        "from langchain.document_loaders import AzureBlobStorageContainerLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "#from azure.core.credentials import AzureKeyCredential\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"a35e69b894d64657ac8fe96a1e98a7d5\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ncus-lfai-nprd-01.openai.azure.com/\"\n",
        "os.environ[\"AZURE_CONN_STRING\"] = \"DefaultEndpointsProtocol=https;AccountName=lfaimvpstorage01;AccountKey=BfDp9OANA7VAsAb/ZKZ0GLxM+vFHEWKB9sBQdWEPQVjTpQghE37A3aDWz+h+XhWIHETs2tw338Xl+AStVk7Saw==;EndpointSuffix=core.windows.net\"\n",
        "os.environ[\"CONTAINER_NAME\"] =\"lf-hr-qna-cont01\"\n",
        "os.environ[\"AZURE_COGNITIVE_SEARCH_SERVICE_NAME\"] = \"lfai-ir-search-service\"\n",
        "\n",
        "vector_store_address:str = f\"https://${os.environ.get('AZURE_COGNITIVE_SEARCH_SERVICE_NAME')}.search.windows.net\"\n",
        "hr_index_name:str = 'lf-hr-qna-index'\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "        openai_api_version=\"2023-05-15\",\n",
        "        azure_deployment=\"gpt-35-turbo-16k\",\n",
        "        temperature=0\n",
        ")\n",
        "\n",
        "az_embeddings = AzureOpenAIEmbeddings()\n",
        "\n",
        "vector_store:AzureSearch = AzureSearch(\n",
        "  azure_search_endpoint = vector_store_address,\n",
        "  azure_search_key = os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
        "  index_name = hr_index_name,\n",
        "  embedding_function = az_embeddings.embed_query\n",
        ")\n",
        "\n",
        "loader = AzureBlobStorageContainerLoader(\n",
        "  con_str = os.environ.get(\"AZURE_CON_STRING\"),\n",
        "  container = os.environ.get(\"CONTAINER_NAME\")\n",
        ")\n",
        "docs = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size = 1024,chunk_overlap = 0)\n",
        "text_chunk = text_splitter.split_documents(docs)\n",
        "vector_store.add_documents(documents = text_chunk)\n",
        "print(\"Data loaded into vector store successfully!\")"
      ],
      "metadata": {
        "id": "SxtS74NJp-oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dHuIMmBaqChZ"
      }
    }
  ]
}