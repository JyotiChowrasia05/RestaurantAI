{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuYmBUIZb5nAVQ1mSR3zLc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JyotiChowrasia05/RestaurantAI/blob/main/RestaurantAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mzB0n9n27YZ",
        "outputId": "f4c65b8c-2ca2-46c1-ea25-992b80c1c2f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.353-py3-none-any.whl (803 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.4/803.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.1/803.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.2 (from langchain)\n",
            "  Downloading langchain_community-0.0.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.4 (from langchain)\n",
            "  Downloading langchain_core-0.1.4-py3-none-any.whl (205 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.7/205.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.70 (from langchain)\n",
            "  Downloading langsmith-0.0.75-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.4->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.4->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.4->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.4->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.353 langchain-community-0.0.7 langchain-core-0.1.4 langsmith-0.0.75 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "id": "NieIz0hJpCWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install OpenAI"
      ],
      "metadata": {
        "id": "GGlfvtyl3BFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install typing_extensions"
      ],
      "metadata": {
        "id": "eKC84adq3Dfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NbgsvDniiGM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import streamlit as st\n",
        "import langchain\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "w9lVKlyyh1uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.retrievers import AzureCognitiveSearchRetriever\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "import json\n",
        "import os, sys\n",
        "import hashlib\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "USMZm1BbAVZf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "42e4f1a4-2ed8-432c-c6de-1929c14b3d8e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8f267954c37b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRetrievalQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrievers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAzureCognitiveSearchRetriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversationalRetrievalChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAzureChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"a35e69b894d64657ac8fe96a1e98a7d5\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ncus-lfai-nprd-01.openai.azure.com/\"\n",
        "os.environ[\"AZURE_COGNITIVE_SEARCH_SERVICE_NAME\"] = \"lfai-ir-search-service\"\n",
        "os.environ[\"AZURE_COGNITIVE_SEARCH_INDEX_NAME\"] = \"ir-vector-index\"\n",
        "os.environ[\"AZURE_COGNITIVE_SEARCH_API_KEY\"] = \"GFhhonWdwKTTxx1hlEQ98erC4pBqErNcgbOVYcAdNWAzSeCwaydv\"\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\""
      ],
      "metadata": {
        "id": "gmd7zs38zGA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = AzureCognitiveSearchRetriever(content_key=\"chunk\", top_k=2, index_name = \"ir-vector-index\")"
      ],
      "metadata": {
        "id": "pp_0-Sv_1zxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = AzureChatOpenAI(\n",
        "          openai_api_version=\"2023-05-15\",\n",
        "          azure_deployment=\"gpt-35-turbo-16k\",\n",
        "          temperature=0\n",
        "  )"
      ],
      "metadata": {
        "id": "5TCyaqjV_ymo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "a7fd253a-f569-4cd7-dd5d-94f9e53d9dce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-76851cb6170a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m llm = AzureChatOpenAI(\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mopenai_api_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2023-05-15\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mazure_deployment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-35-turbo-16k\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AzureChatOpenAI' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables=[\"cuisine\"],\n",
        "    template= \"I want to Open a Resaurant for {cuisine} food\"\n",
        ")\n",
        "name_chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
        "\n",
        "#prompt_template_name.format(cuisine=\"Italian\")\n",
        "\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables = ['restaurant_name'],\n",
        "    template = \"Suggest some Menu Item for {restaurant_name}\"\n",
        ")\n",
        "items_chain = LLMChain(llm=llm, prompt=prompt_template_items)\n",
        "\n",
        "allchains = SimpleSequentialChain(chains=[name_chain,items_chain])"
      ],
      "metadata": {
        "id": "MCeV9chdbWgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = allchains.run('Indian')\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Wkpl102QjVfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mychain.run('German')"
      ],
      "metadata": {
        "id": "MFTFzppghI35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [file for file in glob.glob(\"../somefolder/*\")]\n",
        "for file_name in files:\n",
        "    with io.open(file_name, 'rb') as image_file:\n",
        "        content = image_file.read()"
      ],
      "metadata": {
        "id": "LRS9zV5yDjbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "files = os.listdir('./sample_data')\n",
        "\n",
        "# print(len(arr))\n",
        "# for file in files:\n",
        "#   print(file)"
      ],
      "metadata": {
        "id": "GGGA0AnHFWeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr_txt = [t for t in os.listdir('./sample_data') if t.endswith(\".txt\")]\n",
        "arr_csv = [c for c in os.listdir('./sample_data') if c.endswith(\".csv\")]\n",
        "arr_pdf = [p for p in os.listdir('./sample_data') if p.endswith(\".pdf\")]\n",
        "\n",
        "print(arr_txt)\n",
        "print(arr_csv)\n",
        "print(arr_pdf)"
      ],
      "metadata": {
        "id": "frdJ4pyWIGn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_files(file):\n",
        "  print(\"Total Text files are:\",file)\n",
        "\n",
        "def get_csv_files(file):\n",
        "  print(\"Total CSV files are:\",file)\n",
        "\n",
        "def get_pdf_files(file):\n",
        "  print(\"Total pdf files are:\",file)\n",
        "\n",
        "for file in os.listdir('./sample_data'):\n",
        "  if file.endswith(\".txt\"):\n",
        "    get_text_files(file)\n",
        "    #print(file)\n",
        "  if file.endswith(\".csv\"):\n",
        "    get_csv_files(file)\n",
        "    #print(file)\n",
        "  if file.endswith(\".pdf\"):\n",
        "    get_pdf_files(file)\n",
        "    #print(file)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y_1_GDAfJGzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Mr. Jyoti like eating Samosa. Mr. Sagar like eating Vada Pav\")\n",
        "print(doc[11])"
      ],
      "metadata": {
        "id": "KXdNAPU8gJyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import textfileext as tf ,pdffileext as pf\n",
        "\n",
        "path = 'C:/Users/jyoti.chowrasia/js/handson_apps/sample_files/'\n",
        "hr_qns=[]\n",
        "\n",
        "def get_text_files(file):\n",
        "    text_arr=[]\n",
        "    text_file_obj = open(file,'r',encoding='cp437')\n",
        "    text_doc = text_file_obj.readline()\n",
        "    text_doc_sentence = sent_tokenize(text_doc)\n",
        "    text_file_obj.close()\n",
        "\n",
        "    for i in range(len(text_doc_sentence)):\n",
        "        #response = \"Q\"+ str(i+1) + \": \" + file_sent[i]\n",
        "        response = \"Q: \"+ text_doc_sentence[i]\n",
        "        text_arr.append(response)\n",
        "    return text_arr\n",
        "\n",
        "def get_pdf_file(file):\n",
        "    pdf_arr=[]\n",
        "    pdf_file_obj = open(file,'rb')\n",
        "    pdf_doc = PyPDF2.PdfReader(pdf_file_obj)\n",
        "    page_len = len(pdf_doc.pages)\n",
        "    pdf_doc_page_obj = pdf_doc.pages[7]\n",
        "    pdf_doc_page_extract = pdf_doc_page_obj.extract_text()\n",
        "    #print(page_extract)\n",
        "    pdf_file_obj.close()\n",
        "    pdf_doc_sentence = sent_tokenize(pdf_doc_page_extract)\n",
        "    #print(pdf_doc_sentence)\n",
        "    for i in range(len(pdf_doc_sentence)):\n",
        "        #response = \"Q\"+ str(i+1) + \": \" + file_sent[i]\n",
        "        response = \"Q: \"+ pdf_doc_sentence[i]\n",
        "        pdf_arr.append(response)\n",
        "        #print(arr)\n",
        "    return pdf_arr\n",
        "\n",
        "for file in os.listdir(path):\n",
        "    if file.endswith('.txt'):\n",
        "       #res = tf.get_text_files(path + file)\n",
        "       res = get_text_files(path + file)\n",
        "       hr_qns.append(res)\n",
        "\n",
        "    if file.endswith('.pdf'):\n",
        "       #res = pf.get_pdf_file(path + file)\n",
        "       res = get_pdf_file(path + file)\n",
        "       hr_qns.append(res)\n",
        "\n",
        "    #if file.endswith('.csv'):\n",
        "    #  get_csv_files(path + file)\n",
        "\n",
        "hr_qns_path = path + \"Hr_QnS.txt\"\n",
        "\n",
        "with open(hr_qns_path, \"w+\",encoding='utf-8') as txt_file:\n",
        "    for line in hr_qns:\n",
        "        txt_file.write(\" \".join(line) + \"\\n\")\n",
        "\n",
        "print(f\"The HR QnS has been written to {hr_qns_path}.\")\n",
        "\n",
        "\n",
        "#print(hr_qns[1])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4-xGBWsoBAKO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}